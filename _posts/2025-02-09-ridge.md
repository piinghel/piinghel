---
layout: post
title: "Ridge regression"
date: 2025-02-09
categories: [Quants]
---


[In the previous article](https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html), I introduced the low volatility factor and demonstrated how a simple volatility scaling approach can significantly boost performance. This time, I’m taking it a step further by combining multiple predictors using a multiple linear regression model. I’ll also explore key modeling choices, including normalization techniques and the choice of target variable for training.

Constructing effective stock selection models involves handling the wide variability in stock prices, volumes, and risk profiles. The goal is to rank stocks by their relative performance over the next period (e.g., 20 days), which makes normalization a critical step. By integrating multiple predictors and testing various modeling approaches, this article highlights and explores how including more predictors, different preprocessing methods and the choice of the target variable affects overall strategy performance.


## Feature Engineering

An important point is that we let the model, in this case a linear regression model, define the form of relationships. Since we use a linear model, we limit ourselves to linear relationships and do not include interaction terms for now. Obviously, we need to define our predictive features and also choose a target variable. For the predictive features, we focus on price, volume, market capitalization, and market derived features. To make this more concrete, we compute the following families of features for every stock (around 3300) in our universe:

1. **Momentum Features**
   - Captures the trend-following behavior of stocks.
   - Examples:
     - Lagged returns over short horizons (e.g., 1 to 10 days).
     - Rolling cumulative returns over longer windows (e.g., 21 to 252 days).
     - Moving Average Convergence Divergence (MACD) to identify shifts in momentum.

2. **Volatility Features**
   - Focuses on the risk profile of stocks.
   - Examples:
     - Rolling historical volatility computed over time windows (e.g., 21, 63, or 126 days).
     - Downside and upside volatility, separating negative and positive deviations in price.
     - Average True Range (ATR) for a normalized measure of price range volatility.

3. **Liquidity Features**
   - Assesses trading activity.
   - Examples:
     - Rolling mean and standard deviation of adjusted trading volume.
     - Ratio of current volume to its rolling maximum to highlight unusual trading activity.

4. **Size Features**
   - Measures the size of a stock using market capitalization.
   - Examples:
     - Rolling transformations of market cap, such as mean and minimum values over time.
     - Helps differentiate between small-cap and large-cap stocks.

5. **Short Mean Reversion Features**
   - Identifies conditions where prices revert to a historical mean.
   - Examples:
     - Deviation of the price from its rolling moving average.
     - Price position relative to rolling minimum or maximum values.
     - Bollinger Bands to highlight overbought or oversold conditions.

6. **Correlation with the Market**
   - Captures the systematic risk of a stock by measuring its relationship with the market index.
   - Examples:
     - Rolling correlation with the Russell 1000 over time windows (e.g., 63 days).
     - Useful for identifying defensive stocks or those with high beta.

 In total we have around 150 predictive features, some of them will be very correlated obviously. Next is determining our target variable. I will focus on predicting the return for the next 20 days and predicting the sharpe ratio for the next 20 days. Obviously we could epxlore different periods as well but for the sake of simplicity let's keep it at 20 days.

## Preprocessing Steps

An important step in this process is normalizing the data. To understand why this is necessary, let’s take a step back. The goal is to identify, for each period (e.g., each day), the stocks that are most likely to perform well. We define "performing well" as having the highest return or Sharpe ratio over the next 20 days.

Think of it as creating a ranking: we want to push the best-performing stocks to the top of the list and the worst-performing stocks to the bottom. This means we’re not focused on the actual numbers (like the exact return or Sharpe ratio of each stock); instead, we care about how each stock compares to the others. The problem is relative by nature we want to know if stock A is likely to do better than stock B in terms of sharpe ratio.

To simplify the problem, we normalize the data for each day across all stocks. This process, called cross-sectional normalization, adjusts the predictive features and the target variable so they are on a consistent scale. For example, this could involve scaling the values to have a mean of zero and a standard deviation of one also know as Z-scoring. By doing this, we make it easier for the model to compare stocks and focus on their relative rankings, rather than being influenced by differences in scale or magnitude.

I will compare the following normalization methods:

1. **No normalization (Raw):** Used as a baseline to compare the performance of other methods.
2. **Ranking and mapping to a range (0 to 100):** The lowest value is mapped to 0, and the highest value is mapped to 100.
3. **Z-scoring:** Values are normalized to have a mean of 0 and a standard deviation of 1. To handle outliers, we clip values greater than 5 or less than -5 to 5 and -5, respectively.

Ranking maps all values to a uniform distribution, making the data stationary and naturally handling outliers. This method increases stability by compressing all values into a fixed range, but it also has a downside: some information is lost because the values are "squeezed" into the interval. Z-scoring, on the other hand, provides more freedom by preserving the magnitude of differences between values, while still managing outliers with clipping.

Finally, we will compare one additional approach:

- **Sector-specific ranking of the target variable:** The target variable is ranked within each sector, which is similar to making your stock selection sector-neutral.

These comparisons will help us understand the trade-offs and advantages of different normalization methods.


 For missing data, we use the following logic. We start with doing forward fill, when forward fill is not an option, meaning there are no pevious data points. We will using the cross sectional mean by from the others stock in the same sector for that specific date. When this is not an option, I will impute just by the mean after normalizing the data;  0 for z scoring and 50 for ranking. Obviously this topic on it's own deserved a whole post on itself as it's important and many possibility better approaches exist.



## Model and Validation Procedure

### Generalized Prediction Framework

To fixate the problem, we model the score or ranking of a stock (which can be its **Sharpe ratio, return, or another performance measure**) using an additive prediction error model:

$$
s_{i,t+1} = \mathbb{E}_t[s_{i,t+1}] + \epsilon_{i,t+1},
$$

where

$$
\mathbb{E}_t[s_{i,t+1}] = g(\mathbf{z}_{i,t}).
$$

Stocks are indexed as $i = 1,...,N_t$ and time periods as $t = 1,...,T$. We assume a balanced panel for simplicity, deferring missing data handling to the preprocessing section.

The objective is to model $$\mathbb{E}_t[s_{i,t+1}]$$, the **expected ranking score** of a stock, as a function of its predictor variables. We denote the predictors as the $P$-dimensional vector $\mathbf{z}_{i,t}$, where $g(\cdot)$ is a flexible function mapping these predictors to expected rankings.

In this framework, the function $g(\cdot)$ has two key characteristics:

- **Consistency across stocks and time periods**: The function $g(\cdot)$ does not vary based on the specific stock $i$ or time $t$; it maintains the same form for all stocks and periods. By leveraging information from the entire panel, the model stabilizes ranking estimates, ensuring that the function behaves uniformly across the data.

- **Dependence only on $$\mathbf{z}_{i,t}$$**: The function $g(\cdot)$ relies on the feature vector $ \mathbf{z}_{i,t} $ for each stock $i$ at time $t$, and does not explicitly incorporate information from previous periods or from other stocks. While lagged features are included in the model, the function avoids direct dependencies on past data or cross-stock information.


We approximate $g(\cdot)$ using **Ridge Regression**, a linear model that is particularly useful when predictors are highly correlated. 

### Motivation for Using Ridge Regression  

Before moving on to the validation procedure, let’s briefly highlight why Ridge Regression is well-suited for our approach:  

- It is particularly useful in high-dimensional settings or when predictors are highly correlated, as it mitigates multicollinearity.  
- The $L_2$ penalty prevents extreme coefficient values, leading to a more stable and generalizable model.  
- It helps balance bias and variance, reducing overfitting while preserving important predictive relationships.  

 
The ridge regression loss function is:

$$ 
\text{Minimize: } \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2
$$

With these advantages in mind, we now turn to the validation procedure, where we train the model and assess its performance.  

### Expanding Walkforward Validation Procedure


To test and improve the ridge regression model, we use an **expanding walkforward validation procedure**. It starts with a 3-year burn-in period. After that, the model is updated about every 2 years using all the data available up to that point. This way, the model keeps learning from new data while using the full history for better predictions.

In my experience, using an **expanding walkforward approach**, which includes all available data, works better than rolling validation methods. Rolling validation only uses recent data and may miss important long-term patterns, leading to less stable predictions. The expanding method allows the model to use more data, which helps it find these patterns and make more reliable predictions.

You can also choose to **split the training data into a train and validation set** within each period if you want to do hyperparameter tuning. This isn't required for all cases, but it can help fine-tune the model before testing it, making it more accurate.

Below is a detailed representation of the expanding walkforward procedure:



![Figure 1](/assets/ridge/walk-forward.png)

**Figure 1**: schematic overview of an expanding walk forward procedure.

## Portfolio Construction


We use the same portfolio allocation model as in  [our previous article](https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html). The strategy ranks stocks based on the model’s rankings, selecting the top 75 stocks to go long and the bottom 75 to go short.

To ensure the risk is balanced, we apply a volatility targeting approach. This approach adjusts the weights of stocks according to their risk profile to ensure that each stock within the portfolio contributes approximately the same amount of risk. This way, we achieve a balanced portfolio with consistent risk across the long and short portfolio.

By combining the model’s rankings with a risk-targeting strategy, we create a portfolio that is both well-diversified and dynamically adjust its risk over time.


## Results

To analyze the impact of different modeling choices, I have plotted the cumulative returns of all strategies, rescaled to a volatility of 8% (Figure 2). This ensures a fair comparison by normalizing risk levels across strategies.

In total, we evaluate 10 different modeling choices, combining 5 normalization methods (Unprocessed, Z-score overall, Ranking overall, Z-score by sector, and Ranking by sector) with 2 target labels (SR 20 and Return 20). The goal is to assess how both the normalization approach and the choice of target label influence performance.

For now, I’ve left the strategy labels out—let’s keep some tension before diving into the details. As you can see, there is quite a bit of variation in performance across the different approaches. Below, I present the results and key insights into how these modeling choices affect strategy returns.


![Figure 2](/assets/ridge/all_lines.png)

**Figure 2**: Performance of all different modelling choices. All lines are scaled to the same level of volatility.

Figure 2 explores the impact of different modeling choices on performance, measured in terms of Sharpe ratio. The first comparison looks at normalization methods, evaluating whether ranking, raw values, or z-scoring lead to better results. The second focuses on whether normalization is applied globally or within sectors. The final comparison examines whether training the model on return or Sharpe ratio yields superior outcomes.

The results make one thing clear: ranking within sectors and training the model on Sharpe ratio consistently lead to the best performance. However, the impact of the normalization method itself is less obvious. I initially expected both z-scoring and ranking to outperform raw normalization, but the results don’t show a decisive advantage for either approach.

To dig deeper, Figure 3 provides a more granular view by conditioning performance on the target label. Interestingly, when training on Sharpe ratio, both ranking and z-scoring produce significantly stronger results compared to raw normalization. However, when training on return, z-scoring surprisingly performs the worst, while ranking remains a strong choice.

This contrast suggests that the effectiveness of a normalization method depends heavily on the modeling objective. While z-scoring improves performance when optimizing for Sharpe ratio, it seems detrimental when the model is trained on return. Ranking, on the other hand, appears to be a robust choice across both scenarios, making it a reliable default.



![Figure 3](/assets/ridge/summary_barplot.png)

**Figure 3**: Sharpe Ratio Performance Across Key Modeling Choices.

![Figure 4](/assets/ridge/normalization_target.png)


**Figure 4**: Cumulative return of different normalization methods conditioned on the target label. Performance are scaled to the same level of volatility.


| Model                          | Return (ann. %) | Volatility (ann. %) | Sharpe Ratio (ann.) | Maximum Drawdown (%) |
|--------------------------------|-----------------|---------------------|---------------------|-----------------------|
| sr_zscore_by_sector            | 10.8            | 8.1                 | 1.3                 | 12.6                  |
| combo                          | 8.4             | 6.7                 | 1.3                 | 13.4                  |
| sr_ranking_by_sector           | 9.8             | 8.0                 | 1.2                 | 12.9                  |
| sr_zscore_globally             | 10.1            | 8.6                 | 1.2                 | 19.0                  |
| sr_ranking_globally            | 9.3             | 8.5                 | 1.1                 | 16.9                  |
| sr_raw_globally                | 8.9             | 8.5                 | 1.1                 | 16.1                  |
| return_ranking_by_sector       | 7.6             | 7.6                 | 1.0                 | 14.9                  |
| return_raw_globally            | 7.2             | 7.3                 | 1.0                 | 16.7                  |
| return_ranking_globally        | 7.5             | 7.7                 | 1.0                 | 18.4                  |
| return_zscore_by_sector        | 6.6             | 7.4                 | 0.9                 | 20.1                  |
| return_zscore_globally         | 5.5             | 7.5                 | 0.7                 | 21.0                  |



**Table 1**: Statistics of all different modelling choices. Ranked in ascending order based on sharpe ratio.



## TODO
- results be a bit more in detail about the performance
- Add flowchart for walkfroward; explain it a bit more that you can do an extra validation procedure
- Maybe add the normaliation a bit more clear with a fomula
- Conclude
