---
layout: post
title: "Ridge regression"
date: 2025-02-09
categories: [Quants]
---


In the previous article, I introduced the low volatility factor and demonstrated how a simple volatility scaling approach can significantly boost performance. This time, I’m taking it a step further by combining multiple predictors using a multiple linear regression model. I’ll also explore key modeling decisions, including normalization techniques and the choice of target variable for training.

Constructing effective stock selection models involves handling the wide variability in stock prices, volumes, and risk profiles. The goal is to rank stocks by their relative performance over the next period (e.g., 20 days), which makes normalization a critical step. By integrating multiple predictors and testing various modeling approaches, this article highlights how including more predictors and different preprocessing methods can improve overall strategy performance.


## Feature Engineering

An important point is that we let the model, in this case a linear regression model, define the form of relationships. Since we use a linear model, we limit ourselves to linear relationships and do not include interaction terms for now. Obviously, we need to define our predictive features and also choose a target variable. For the predictive features, we focus on price, volume, market capitalization, and market derived features. To make this more concrete, we compute the following families of features for every stock (around 3300) in our universe:

1. **Momentum Features**
   - Captures the trend-following behavior of stocks.
   - Examples:
     - Lagged returns over short horizons (e.g., 1 to 10 days).
     - Rolling cumulative returns over longer windows (e.g., 21 to 252 days).
     - Moving Average Convergence Divergence (MACD) to identify shifts in momentum.

2. **Volatility Features**
   - Focuses on the risk profile of stocks.
   - Examples:
     - Rolling historical volatility computed over time windows (e.g., 21, 63, or 126 days).
     - Downside and upside volatility, separating negative and positive deviations in price.
     - Average True Range (ATR) for a normalized measure of price range volatility.

3. **Liquidity Features**
   - Assesses trading activity.
   - Examples:
     - Rolling mean and standard deviation of adjusted trading volume.
     - Ratio of current volume to its rolling maximum to highlight unusual trading activity.

4. **Size Features**
   - Measures the size of a stock using market capitalization.
   - Examples:
     - Rolling transformations of market cap, such as mean and minimum values over time.
     - Helps differentiate between small-cap and large-cap stocks.

5. **Short Mean Reversion Features**
   - Identifies conditions where prices revert to a historical mean.
   - Examples:
     - Deviation of the price from its rolling moving average.
     - Price position relative to rolling minimum or maximum values.
     - Bollinger Bands to highlight overbought or oversold conditions.

6. **Correlation with the Market**
   - Captures the systematic risk of a stock by measuring its relationship with the market index.
   - Examples:
     - Rolling correlation with the Russell 1000 over time windows (e.g., 63 days).
     - Useful for identifying defensive stocks or those with high beta.

 In total we have around 150 predictive features, some of them will be very correlated obviously. Next is determining our target variable. I will focus on predicting the return for the next 20 days and predicting the sharpe ratio for the next 20 days. Obviously we could epxlore different periods as well but for the sake of simplicity let's keep it at 20 days.

## Preprocessing Steps

An important step in this process is normalizing the data. To understand why this is necessary, let’s take a step back. The goal is to identify, for each period (e.g., each day), the stocks that are most likely to perform well. We define "performing well" as having the highest return or Sharpe ratio over the next 20 days.

Think of it as creating a ranking: we want to push the best-performing stocks to the top of the list and the worst-performing stocks to the bottom. This means we’re not focused on the actual numbers (like the exact return or Sharpe ratio of each stock); instead, we care about how each stock compares to the others. In other words, the problem is relative—we only need to know if stock A is likely to do better than stock B.

To simplify the problem, we normalize the data for each day across all stocks. This process, called cross-sectional normalization, adjusts the predictive features and the target variable so they are on a consistent scale. For example, this could involve scaling the values to have a mean of zero and a standard deviation of one also know as Z-scoring. By doing this, we make it easier for the model to compare stocks and focus on their relative rankings, rather than being influenced by differences in scale or magnitude.

I will compare the following normalization methods:

1. **No normalization (Raw):** Used as a baseline to compare the performance of other methods.
2. **Ranking and mapping to a range (0 to 100):** The lowest value is mapped to 0, and the highest value is mapped to 100.
3. **Z-scoring:** Values are normalized to have a mean of 0 and a standard deviation of 1. To handle outliers, we clip values greater than 5 or less than -5 to 5 and -5, respectively.

Ranking maps all values to a uniform distribution, making the data stationary and naturally handling outliers. This method increases stability by compressing all values into a fixed range, but it also has a downside: some information is lost because the values are "squeezed" into the interval. Z-scoring, on the other hand, provides more freedom by preserving the magnitude of differences between values, while still managing outliers with clipping.

Finally, we will compare one additional approach:

- **Sector-specific ranking of the target variable:** The target variable is ranked within each sector, which is similar to making your stock selection sector-neutral.

These comparisons will help us understand the trade-offs and advantages of different normalization methods.


Maybe very briefly. For missing data, we use the following logic. We start with doing forward fill, when forward fill is not an option, meaning there are no pevious data points. We will using the cross sectional mean by from the others stock in the same sector for that specific date. When this is not an option, I will impute just by the mean after normalizing the data;  0 for z scoring and 50 for ranking. Obviously this topic on it's own deserved a whole post on itself as it's important and many possibility better approaches exist.



## Model and Validation Procedure

### Motivation for Using Ridge Regression

So, we have defined our preprocessing steps, let's move on to our validation procedure to train our linear regression model. 
s
- Ridge regression is ideal when predictors are highly correlated or when the dataset has more features than observations, as it addresses multicollinearity effectively.  
- By adding an $L_2$ penalty to the loss function, it stabilizes coefficient estimates, improving the model's robustness and predictive accuracy.  
- It reduces overfitting by shrinking coefficients, striking a balance between bias and variance.  
 

The ridge regression loss function is:

$$ 
\text{Minimize: } \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2
$$
 

### Expanding Walkforward Validation Procedure

To evaluate and refine the ridge regression model, we use an **expanding walkforward validation procedure** with a burn-in period of 3 years. After the initial period, the model is refit approximately every 2 years using all the data available up to that point. This approach ensures the model continuously adapts to new information while leveraging the full historical dataset for more robust predictions.

In my experience, using an **expanding walkforward approach**—which incorporates all available data—consistently outperforms rolling walkforward validation methods. Rolling validation, which limits the training window to recent data, may miss valuable long-term patterns and lead to less stable predictions.

Below is a detailed representation of the expanding walkforward procedure:



## Portfolio Construction


For portfolio construction, we follow a systematic approach to leverage the model's predictions effectively. Each day, the model assigns a prediction score (or ranking) to every stock in our universe, with higher scores corresponding to higher expected performance and lower scores indicating lower expected performance.

To build the portfolio:
1. We select an **arbitrary number of stocks** for the long and short sides. For this article, we choose **75 stocks with the highest scores** to go long (buy) and **75 stocks with the lowest scores** to go short (sell). Numbers like 50, 100, or 150 would work equally well and would not materially affect our conclusions.
2. We apply a **volatility targeting approach** at the stock level to ensure that each stock contributes approximately the same amount of risk to the portfolio. This step makes sure every stock has a similar risk contribution.

By combining the model's predictions with risk-aware allocation, we create a balanced portfolio that maximizes the use of available information while maintaining a consistent risk profile.

---

## Results

So in total we will have 10 different modelling choices; 5 on the normalization(Raw, Z-score overall, Ranking overall, Z score by industry, and Ranking by industry) and 2 on the target label (SR 20 and Return 20). Below I've plotted performance for all of them and removed labels to keep some tension hhihi. As you can see there is quite some variation in performance.

Let's take a closer look if we find some superior modelling choices. Starting by looking at the whether we should normalize and if so which one performs better (Ranking or Z score). Below I've visualized the cumultative performance for the normalization methods. I've rescaled volatility to 10 % in the graph and I also show some key performance metrics. Clearly normalization helps, both z score and ranking performing better than the raw (no normaiztaion). 

So I have listed a table before of the 10 different modelling combinations as well as included the combo which is the average off all combinations. Models are ranked in desceding order based on sharpe ratio. To make the patterns a bit more clear I've aggregate performance based on sharpe ratio based on the 3 modelling choices; normalizatoin, target ranked within sector or global ranking and finally target label sharpe ratio or return. It's very clearly that ranking within sector and training the model on sharpe ratio are the superior choice here. For the  normalization part it's not clear. I did expect the both z scoring and ranking to outperform here. But let's zoom in a bit and look at performance a bit more closely. In figures below I visualize performance for the normaization techniques by further conditoin on the target label. It's interesting to see now that for the sharpe ratio both ranking and z scroing clearly perform better here whereras when the model is training on return this is not the case.

Finally, let's look at our target label SR or Return. Sharpe ratio here is the better choice. 

![Figure 1](/assets/ridge/all_lines.png)

**Figure 1**: Performance of all different modelling choices. All lin es are scaled to the same level of volatility.




![Figure 2](/assets/ridge/summary_barplot.png)

![Figure 3](/assets/ridge/normalization_target.png)


**Figure 3**: Barplot.


| Model                          | Return (ann. %) | Volatility (ann. %) | Sharpe Ratio (ann.) | Maximum Drawdown (%) |
|--------------------------------|-----------------|---------------------|---------------------|-----------------------|
| sr_zscore_by_sector            | 10.82           | 8.07                | 1.34                | 12.56                 |
| combo                          | 8.41            | 6.69                | 1.26                | 13.41                 |
| sr_ranking_by_sector           | 9.82            | 7.97                | 1.23                | 12.85                 |
| sr_zscore_globally             | 10.14           | 8.55                | 1.19                | 19.00                 |
| sr_ranking_globally            | 9.26            | 8.54                | 1.08                | 16.91                 |
| sr_raw_globally                | 8.94            | 8.54                | 1.05                | 16.12                 |
| return_ranking_by_sector       | 7.57            | 7.62                | 0.99                | 14.85                 |
| return_raw_globally            | 7.19            | 7.28                | 0.99                | 16.72                 |
| return_ranking_globally        | 7.45            | 7.67                | 0.97                | 18.41                 |
| return_zscore_by_sector        | 6.62            | 7.40                | 0.89                | 20.13                 |
| return_zscore_globally         | 5.45            | 7.47                | 0.73                | 20.97                 |


**Table 1**: Statistics of all different modelling choices. Ranked in ascending order based on sharpe ratio.



## TODO
- How to make my point for the normalization
- include graphs
- results be a bit more in detail about the performance
