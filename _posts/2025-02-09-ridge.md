---
layout: post
title: "Ridge regression"
date: 2025-02-09
categories: [Quants]
---


[In the last article](https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html), we saw how a simple **low-volatility ranking strategy** delivered solid returns. By scaling returns based on volatility, we could improve performance with minimal complexity. But what if we could do better?  

Rather than relying on a single heuristic, why not let the data determine the optimal ranking function? That’s exactly what we’ll do here—**combine multiple features** and use **a linear regression model** to rank stocks based on expected performance.  

Stock selection isn’t straightforward. We deal with **price changes, trading volume, and risk levels**, all measured on different scales. Without proper normalization, large features like market cap will overshadow smaller ones like short-term returns. In the previous approach, we used a **single low-volatility rule** to rank stocks, which worked well. This time, we’ll refine the process by using **multiple linear regression** to integrate several predictive signals into one model.  

The dataset remains the same—**daily price, volume, and market capitalization data** for all **Russell 1000** constituents, covering **3,300 stocks historically**. Since the investable universe changes over time, we apply basic liquidity filters, excluding stocks priced below **$5** to ensure realistic implementation.  



## Feature Engineering

A key advantage of using a regression model is that it learns how to combine features on its own. Since we’re using **linear regression**, we’re assuming all relationships are **linear**—no interaction terms for now.  

Obviously, we need to define our predictive features and choose a target variable. We focus on **price, volume, market capitalization, and market-derived features**. Below is a breakdown of the main feature groups, computed daily for all **3,300 stocks** in our universe.  

1. **Momentum Features**  
   - Capture trend-following behavior.  
   - Examples:  
     - Lagged returns over 1 to 10 days.  
     - Rolling cumulative returns over 21 to 252 days.  
     - Moving Average Convergence Divergence (MACD) to detect shifts in momentum.  

2. **Volatility Features**  
   - Measure risk.  
   - Examples:  
     - Rolling historical volatility over 21, 63, or 126 days.  
     - Separate downside and upside volatility.  
     - Average True Range (ATR) to normalize price fluctuations.  

3. **Liquidity Features**  
   - Assess trading activity.  
   - Examples:  
     - Rolling mean and standard deviation of trading volume.  
     - Ratio of current volume to its rolling maximum to highlight unusual trading activity.  

4. **Size Features**  
   - Measure company scale.  
   - Examples:  
     - Rolling mean and minimum of market cap.  
     - Distinguishes small-cap from large-cap stocks.  

5. **Short Mean Reversion Features**  
   - Identify when prices revert to historical norms.  
   - Examples:  
     - Price deviation from its rolling moving average.  
     - Position relative to rolling minimum and maximum values.  
     - Bollinger Bands to spot overbought or oversold conditions.  

6. **Correlation with the Market**  
   - Capture systematic risk.  
   - Examples:  
     - Rolling correlation with the Russell 1000 over 63-day windows.  
     - Helps separate defensive stocks from high-beta names.  

That leaves us with **around 150 predictive features**, some of which are obviously correlated.  

### Target Variable
Now, what exactly are we trying to predict? We’ll focus on two targets:  

- **Return over the next 20 days**  
- **Sharpe ratio over the next 20 days**  

Could we explore other horizons? Sure. But to keep things simple, we’ll stick to **20 days** for now.  


## Preprocessing: Normalizing the Data 

Stock features come in all shapes and sizes. Some, like market cap, have values in the billions, while others, like daily returns, are tiny. **The model will still work without normalization**, but features on vastly different scales might lead to **weird or unintended rankings**.  

The intuition is simple: putting everything on a similar scale **should** improve ranking consistency. But is that actually true? That’s what we’re testing here.  

To do this, we apply **cross-sectional normalization**, meaning we adjust each feature **relative to all other stocks on the same day**. That way, the model focuses on **relative differences** rather than raw values.  


For a given feature $X$, the normalized value for stock $i$ on time $t$ is:  

$$
X^{\text{norm}}_{i,t} = f(X_{i,t}, X_{1:N,t})
$$  

where:  
- $X_{i,t}$ is the raw feature value for stock $i$ at time $t$.  
- $X_{1:N,t}$ is the set of values for all stocks on time $t$.  
- $f(\cdot)$ is the chosen normalization method.  

There are a few ways to do this:  

#### **1. Z-Scoring**  
Z-scoring transforms values so they have a **mean of 0** and a **standard deviation of 1**:  

$$
X^{\text{norm}}_{i,t} = \frac{X_{i,t} - \mu_t}{\sigma_t}
$$  

where:  
- $\mu_t$ is the mean across all stocks on day $t$:  

  $$
  \mu_t = \frac{1}{N} \sum_{i=1}^{N} X_{i,t}
  $$  

- $\sigma_t$ is the standard deviation:  

  $$
  \sigma_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_{i,t} - \mu_t)^2}
  $$  

To prevent extreme values from distorting results, we clip any outliers beyond **±5 standard deviations**.  

#### **2. Ranking Normalization**  
Instead of working with raw values, we **rank stocks** and scale the ranks between **0 and 1**:  

$$
R_{i,t} = \frac{r_{i,t}}{N}
$$  

where:  
- $r_{i,t}$ is the rank of stock $i$ at time $t$ (1 for the lowest value, $N$ for the highest).  
- $R_{i,t}$ is the normalized rank.  

This method is **less sensitive to extreme values** and works well for features with skewed distributions.  

### **Does Normalization Actually Help?**  

The model doesn’t **need** normalization to work—it will still produce rankings using raw features. But does normalization lead to **better** rankings? That’s what we’re testing.  

We compare four approaches:  

1. **No Normalization (Raw Features)** – A baseline.  
2. **Z-Scoring (Global)** – Standardizes each feature across all stocks.  
3. **Ranking (Global)** – Maps feature values to a uniform scale.  
4. **Sector-Specific Normalization** – Normalizes features **within each sector** instead of across the entire market.  

### Why This Matters

Without normalization, features like **market cap** might completely overshadow others like **short-term returns**. The question is whether that distorts the model’s ability to rank stocks effectively.  

Below, in **Figure 1**, we show how different normalization methods affect a single feature (20-day return). From left to right: the original distribution, Z-scored, and ranked.  

![Figure 1](/assets/ridge/example_normalization.png)  

**Figure 1**: Effect of Normalization on 20-Day Return Distribution. Left: Original data, Middle: Z-scored, Right: Ranked between 0 and 1.  


### Handling Missing Data


Stock data isn’t always perfect—sometimes values are missing. The model can still run without them, but leaving gaps in the data isn’t ideal. So how do we fill them in?  

We keep it simple:  

- **Forward fill:** If a stock has prior data, we use the last known value.  
- **Cross-sectional mean imputation:** If no past data exists, we replace the missing value with the **sector average** for that day.  
- **Default values:** If neither of the above works, we apply:  
  - **Z-scoring:** Missing values are set to `0`.  
  - **Ranking:** Missing values are set to `0.5` (midpoint of the ranking scale).  

Could we get more sophisticated? Absolutely. There are plenty of ways to handle missing data, from advanced statistical methods to ML-based imputations. But for now, we’re sticking with a **simple, reliable approach**.  

A deeper dive into missing data strategies is a topic for another time.  



## Modeling the Ranking Score 

At the core of this strategy is a model that predicts a stock’s **ranking score**—which could be its **Sharpe ratio, return, or any other performance measure**.  

We frame this as a **prediction problem**:  

$$
s_{i,t+1} = \mathbb{E}_t[s_{i,t+1}] + \epsilon_{i,t+1}
$$  

where:  
- $s_{i,t+1}$ is the **true ranking score** for stock $i$ at time $t+1$.  
- $\mathbb{E}_t[s_{i,t+1}]$ is the **model’s expected ranking score** given available information at time $t$.  
- $\epsilon_{i,t+1}$ is the **error term**—things the model can’t predict.  

The goal is to model:  

$$
\mathbb{E}_t[s_{i,t+1}] = g(\mathbf{z}_{i,t})
$$  

where:  
- $\mathbf{z}_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$.  
- $g(\cdot)$ is a function mapping predictors to ranking scores.  

This function $g(\cdot)$ follows two key rules:  

1. **It’s consistent across stocks and time.**  
   - The same function applies to every stock and every time period.  
   - This ensures stable and comparable rankings across the dataset.  

2. **It only depends on the features $\mathbf{z}_{i,t}$.**  
   - The model only considers a stock’s own characteristics at time $t$.  
   - No direct dependencies on past predictions or other stocks.  

### Why Ridge Regression? 

We approximate $g(\cdot)$ using **Ridge Regression**, a linear model that works well when predictors are highly correlated.  

What makes Ridge a good fit?  

- **Handles multicollinearity** – Stocks with similar characteristics tend to move together. Ridge prevents unstable coefficients.  
- **Adds an $L_2$ penalty** – Shrinks large coefficients, reducing overfitting and improving generalization.  
- **Balances bias and variance** – Helps the model stay stable across different time periods.  

The loss function for Ridge Regression is:  

$$ 
\text{Minimize: } \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2
$$  

The second term ($\lambda \sum_{j=1}^p \beta_j^2$) **penalizes large coefficients**, keeping things smooth and stable.  

With the model set up, the next step is **validation**—training Ridge Regression on historical data and evaluating its ability to rank stocks effectively.  


## Expanding Walkforward Validation  

To evaluate the model, we use an **expanding walkforward validation procedure**. It works like this:  

1. **Start with a 3-year burn-in period** – The model isn’t tested yet; it just learns from the data.  
2. **Update the model every 2 years** – Each update incorporates all available data up to that point.  
3. **Keep expanding the dataset** – Older data stays in, while new data gets added.  

This approach ensures the model **keeps learning from new market conditions** while retaining long-term patterns.  

Why not use a **rolling validation window**? Because it **only keeps recent data**, discarding older information that could still be useful. In my experience, an **expanding** window works better because it helps the model recognize long-term relationships, leading to more stable predictions.  

For hyperparameter tuning, you can **split each training period into train and validation sets**. It’s not always necessary, but it helps fine-tune parameters before making predictions.  

Below is a **schematic of the expanding walkforward approach**:  

![Figure 2](/assets/ridge/walk-forward.png)  

**Figure 2**: Expanding walkforward validation process.  


## Portfolio Construction

Once we have stock rankings, we **build a long-short portfolio**:  

- **Go long** on the top 75 ranked stocks.  
- **Short** the bottom 75 ranked stocks.  

Would different numbers (e.g., 50, 100, 150) change the outcome? Not really—the approach is robust across portfolio sizes.  

To balance risk, we apply **volatility targeting**. This means:  
- **Higher-volatility stocks get lower weights.**  
- **Lower-volatility stocks get higher weights.**  

This ensures the portfolio maintains a **consistent risk profile**, rather than being dominated by volatile stocks.  

By combining **data-driven rankings** with **risk-aware position sizing**, we get a **well-diversified portfolio** that dynamically adjusts over time.  


## Results



To compare different modeling choices, we plot the **cumulative returns of all strategies**, scaled to **8% volatility** (Figure 3). This ensures a fair comparison by normalizing risk across models.  

We evaluate **10 different model variations**, combining:  
- **5 normalization methods**: Raw, Z-score (global & sector), Ranking (global & sector).  
- **2 target labels**: Sharpe Ratio (SR 20) and Return (Return 20).  
- **A combined strategy ("Combo")**, which equally weights all strategies.  

For now, Figure 3 remains unlabeled—but what’s clear is that some strategies perform significantly better than others.  

![Figure 3](/assets/ridge/all_lines.png)  

**Figure 3**: Performance comparison of different modeling choices, all scaled to 8% volatility.  


## The biggest impact comes from the target label  

Figure 4 breaks down performance across three key dimensions:  
- **Target label**: Are models trained on **Sharpe Ratio** more effective than those trained on **raw returns**?  
- **Sector-based normalization**: Does normalizing within industries improve stock rankings?  
- **Normalization method**: Does ranking or Z-scoring help relative to using raw features?  

![Figure 4](/assets/ridge/summary_barplot.png)  

**Figure 4**: Sharpe Ratio performance across key modeling choices.  

The results are **not what we might have expected**.  

- **The most important factor is the target label**—models trained on **Sharpe Ratio significantly outperform those trained on raw returns**. This suggests that risk-adjusted rankings provide more stable predictive signals.  
- **Sector-based normalization improves performance**, but to a lesser degree. It still provides a clear benefit over global normalization, reinforcing that stocks should be compared within their peer groups.  
- **Surprisingly, the choice of normalization method (ranking or Z-scoring) doesn’t make a huge difference**. We expected standardization to matter more, but raw features perform nearly as well, suggesting that the model is already capturing key relationships.  

While this gives a high-level overview, Figure 5 dives deeper into **why the target label interacts with normalization choices in unexpected ways**.  



## Normalization effects depend on the target label  

Figure 5 examines **how normalization impacts performance depending on whether the model is trained on Sharpe Ratio or raw returns**.  

![Figure 5](/assets/ridge/normalization_target.png)  

**Figure 5**: Cumulative return of different normalization methods, conditioned on the target label.  

- **For Sharpe Ratio (right panel), both ranking and Z-scoring perform well.**  
  - Sharpe Ratio already accounts for risk, so standardizing features helps extract meaningful signals.  
  - Raw (unprocessed) features lag behind, as expected.  

- **For Return (left panel), normalization makes a much bigger difference.**  
  - Ranking is the best choice, while Z-scoring surprisingly **performs worse than raw features**.  
  - This suggests that return-based training is **more sensitive to normalization artifacts**, possibly because extreme values distort the model.  

The takeaway: **target label is the primary driver of differences, followed by sector normalization, while normalization type itself has surprisingly little impact**.  



## The "Combo" strategy performs surprisingly well  

One unexpected finding is that the **"Combo" strategy**, which equally weights all models, performs almost as well as the best individual model.  

- Despite not being optimized for a single approach, **blending multiple signals leads to robust performance.**  
- The Combo strategy ranks **second overall in terms of Sharpe Ratio**, highlighting the benefits of diversification.  

Instead of searching for a single "best" normalization method, this suggests that **combining multiple perspectives can be a stronger approach**.  



## Full performance breakdown  

To quantify these findings, Table 1 presents a full breakdown of returns, volatility, and Sharpe Ratios across all models.  

| Model                     | Return (ann. %) | Volatility (ann. %) | Sharpe Ratio (ann.) | Maximum Drawdown (%) |
|---------------------------|-----------------|---------------------|---------------------|-----------------------|
| SR Zscore By Sector       | 10.8            | 8.1                 | 1.3                 | 12.6                  |
| Combo                     | 8.4             | 6.7                 | 1.3                 | 13.4                  |
| SR Ranking By Sector      | 9.8             | 8.0                 | 1.2                 | 12.9                  |
| SR Zscore Globally        | 10.1            | 8.6                 | 1.2                 | 19.0                  |
| SR Ranking Globally       | 9.3             | 8.5                 | 1.1                 | 16.9                  |
| SR Raw Globally           | 8.9             | 8.5                 | 1.1                 | 16.1                  |
| Return Ranking By Sector  | 7.6             | 7.6                 | 1.0                 | 14.9                  |
| Return Raw Globally       | 7.2             | 7.3                 | 1.0                 | 16.7                  |
| Return Ranking Globally   | 7.5             | 7.7                 | 1.0                 | 18.4                  |
| Return Zscore By Sector   | 6.6             | 7.4                 | 0.9                 | 20.1                  |
| Return Zscore Globally    | 5.5             | 7.5                 | 0.7                 | 21.0                  |

**Table 1**: Performance metrics across different modeling choices, ranked by Sharpe Ratio. Results exclude transaction costs and slippage.  


## Final thoughts  

- **The biggest driver of performance is the target label.** Training on **Sharpe Ratio** leads to significantly better results than training on returns.  
- **Sector-based normalization improves rankings.** Normalization stocks within their industry provides more stable and accurate signals than global normalization.  
- Normalization method (**ranking vs. Z-scoring**) has **less impact than expected**. Raw features perform nearly as well, suggesting the model already captures key relationships.  
- The **"Combo" strategy performs surprisingly well**, ranking second overall. Diversifying across different approaches stabilizes performance.  
- Instead of searching for a single "best" model, **combining multiple perspectives provides a more robust and stable approach.**  

The key takeaway? **The target label matters most, followed by sector-based normalization. Normalization method itself is less critical than expected.**  

 

## TODO
- results be a bit more in detail about the performance; talk also about the combo
- Maybe add the normaliation a bit more clear with a fomula
- Conclude
- Feature engineering is a bit weird with already speaking about the model without having introducted the model
