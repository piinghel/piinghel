<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-18T15:33:50+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pieter-Jan</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><entry><title type="html">Ridge regression</title><link href="http://localhost:4000/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Ridge regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/09/ridge.html"><![CDATA[<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the previous article</a>, I introduced the low volatility factor and demonstrated how a simple volatility scaling approach can significantly boost performance. This time, I’m taking it a step further by combining multiple predictors using a multiple linear regression model. I’ll also explore key modeling choices, including normalization techniques and the choice of target variable for training.</p>

<p>Constructing effective stock selection models involves handling the wide variability in stock prices, volumes, and risk profiles. The goal is to rank stocks by their relative performance over the next period (e.g., 20 days), which makes normalization a critical step. By integrating multiple predictors and testing various modeling approaches, this article highlights and explores how including more predictors, different preprocessing methods and the choice of the target variable affects overall strategy performance.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>An important point is that we let the model, in this case a linear regression model, define the form of relationships. Since we use a linear model, we limit ourselves to linear relationships and do not include interaction terms for now. Obviously, we need to define our predictive features and also choose a target variable. For the predictive features, we focus on price, volume, market capitalization, and market derived features. To make this more concrete, we compute the following families of features for every stock (around 3300) in our universe:</p>

<ol>
  <li><strong>Momentum Features</strong>
    <ul>
      <li>Captures the trend-following behavior of stocks.</li>
      <li>Examples:
        <ul>
          <li>Lagged returns over short horizons (e.g., 1 to 10 days).</li>
          <li>Rolling cumulative returns over longer windows (e.g., 21 to 252 days).</li>
          <li>Moving Average Convergence Divergence (MACD) to identify shifts in momentum.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Volatility Features</strong>
    <ul>
      <li>Focuses on the risk profile of stocks.</li>
      <li>Examples:
        <ul>
          <li>Rolling historical volatility computed over time windows (e.g., 21, 63, or 126 days).</li>
          <li>Downside and upside volatility, separating negative and positive deviations in price.</li>
          <li>Average True Range (ATR) for a normalized measure of price range volatility.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Liquidity Features</strong>
    <ul>
      <li>Assesses trading activity.</li>
      <li>Examples:
        <ul>
          <li>Rolling mean and standard deviation of adjusted trading volume.</li>
          <li>Ratio of current volume to its rolling maximum to highlight unusual trading activity.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Size Features</strong>
    <ul>
      <li>Measures the size of a stock using market capitalization.</li>
      <li>Examples:
        <ul>
          <li>Rolling transformations of market cap, such as mean and minimum values over time.</li>
          <li>Helps differentiate between small-cap and large-cap stocks.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Short Mean Reversion Features</strong>
    <ul>
      <li>Identifies conditions where prices revert to a historical mean.</li>
      <li>Examples:
        <ul>
          <li>Deviation of the price from its rolling moving average.</li>
          <li>Price position relative to rolling minimum or maximum values.</li>
          <li>Bollinger Bands to highlight overbought or oversold conditions.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Correlation with the Market</strong>
    <ul>
      <li>Captures the systematic risk of a stock by measuring its relationship with the market index.</li>
      <li>Examples:
        <ul>
          <li>Rolling correlation with the Russell 1000 over time windows (e.g., 63 days).</li>
          <li>Useful for identifying defensive stocks or those with high beta.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>In total we have around 150 predictive features, some of them will be very correlated obviously. Next is determining our target variable. I will focus on predicting the return for the next 20 days and predicting the sharpe ratio for the next 20 days. Obviously we could epxlore different periods as well but for the sake of simplicity let’s keep it at 20 days.</p>

<h2 id="preprocessing-steps">Preprocessing Steps</h2>

<p>An important step in this process is normalizing the data. To understand why this is necessary, let’s take a step back. The goal is to identify, for each period (e.g., each day), the stocks that are most likely to perform well. We define “performing well” as having the highest return or Sharpe ratio over the next 20 days.</p>

<p>Think of it as creating a ranking: we want to push the best-performing stocks to the top of the list and the worst-performing stocks to the bottom. This means we’re not focused on the actual numbers (like the exact return or Sharpe ratio of each stock); instead, we care about how each stock compares to the others. The problem is relative by nature we want to know if stock A is likely to do better than stock B in terms of sharpe ratio.</p>

<p>To simplify the problem, we normalize the data for each day across all stocks. This process, called cross-sectional normalization, adjusts the predictive features and the target variable so they are on a consistent scale. For example, this could involve scaling the values to have a mean of zero and a standard deviation of one also know as Z-scoring. By doing this, we make it easier for the model to compare stocks and focus on their relative rankings, rather than being influenced by differences in scale or magnitude.</p>

<p>I will compare the following normalization methods:</p>

<ol>
  <li><strong>No normalization (Raw):</strong> Used as a baseline to compare the performance of other methods.</li>
  <li><strong>Ranking and mapping to a range (0 to 100):</strong> The lowest value is mapped to 0, and the highest value is mapped to 100.</li>
  <li><strong>Z-scoring:</strong> Values are normalized to have a mean of 0 and a standard deviation of 1. To handle outliers, we clip values greater than 5 or less than -5 to 5 and -5, respectively.</li>
</ol>

<p>Ranking maps all values to a uniform distribution, making the data stationary and naturally handling outliers. This method increases stability by compressing all values into a fixed range, but it also has a downside: some information is lost because the values are “squeezed” into the interval. Z-scoring, on the other hand, provides more freedom by preserving the magnitude of differences between values, while still managing outliers with clipping.</p>

<p>Finally, we will compare one additional approach:</p>

<ul>
  <li><strong>Sector-specific ranking of the target variable:</strong> The target variable is ranked within each sector, which is similar to making your stock selection sector-neutral.</li>
</ul>

<p>These comparisons will help us understand the trade-offs and advantages of different normalization methods.</p>

<p>For missing data, we use the following logic. We start with doing forward fill, when forward fill is not an option, meaning there are no pevious data points. We will using the cross sectional mean by from the others stock in the same sector for that specific date. When this is not an option, I will impute just by the mean after normalizing the data;  0 for z scoring and 50 for ranking. Obviously this topic on it’s own deserved a whole post on itself as it’s important and many possibility better approaches exist.</p>

<h2 id="model-and-validation-procedure">Model and Validation Procedure</h2>

<h3 id="generalized-prediction-framework">Generalized Prediction Framework</h3>

<p>To fixate the problem, we model the score or ranking of a stock (which can be its <strong>Sharpe ratio, return, or another performance measure</strong>) using an additive prediction error model:</p>

\[s_{i,t+1} = \mathbb{E}_t[s_{i,t+1}] + \epsilon_{i,t+1},\]

<p>where</p>

\[\mathbb{E}_t[s_{i,t+1}] = g(\mathbf{z}_{i,t}).\]

<p>Stocks are indexed as $i = 1,…,N_t$ and time periods as $t = 1,…,T$. We assume a balanced panel for simplicity, deferring missing data handling to the preprocessing section.</p>

<p>The objective is to model \(\mathbb{E}_t[s_{i,t+1}]\), the <strong>expected ranking score</strong> of a stock, as a function of its predictor variables. We denote the predictors as the $P$-dimensional vector $\mathbf{z}_{i,t}$, where $g(\cdot)$ is a flexible function mapping these predictors to expected rankings.</p>

<p>In this framework, the function $g(\cdot)$ has two key characteristics:</p>

<ul>
  <li>
    <p><strong>Consistency across stocks and time periods</strong>: The function $g(\cdot)$ does not vary based on the specific stock $i$ or time $t$; it maintains the same form for all stocks and periods. By leveraging information from the entire panel, the model stabilizes ranking estimates, ensuring that the function behaves uniformly across the data.</p>
  </li>
  <li>
    <p><strong>Dependence only on \(\mathbf{z}_{i,t}\)</strong>: The function $g(\cdot)$ relies on the feature vector $ \mathbf{z}_{i,t} $ for each stock $i$ at time $t$, and does not explicitly incorporate information from previous periods or from other stocks. While lagged features are included in the model, the function avoids direct dependencies on past data or cross-stock information.</p>
  </li>
</ul>

<p>We approximate $g(\cdot)$ using <strong>Ridge Regression</strong>, a linear model that is particularly useful when predictors are highly correlated.</p>

<h3 id="motivation-for-using-ridge-regression">Motivation for Using Ridge Regression</h3>

<p>Before moving on to the validation procedure, let’s briefly highlight why Ridge Regression is well-suited for our approach:</p>

<ul>
  <li>It is particularly useful in high-dimensional settings or when predictors are highly correlated, as it mitigates multicollinearity.</li>
  <li>The $L_2$ penalty prevents extreme coefficient values, leading to a more stable and generalizable model.</li>
  <li>It helps balance bias and variance, reducing overfitting while preserving important predictive relationships.</li>
</ul>

<p>The ridge regression loss function is:</p>

\[\text{Minimize: } \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>With these advantages in mind, we now turn to the validation procedure, where we train the model and assess its performance.</p>

<h3 id="expanding-walkforward-validation-procedure">Expanding Walkforward Validation Procedure</h3>

<p>To test and improve the ridge regression model, we use an <strong>expanding walkforward validation procedure</strong>. It starts with a 3-year burn-in period. After that, the model is updated about every 2 years using all the data available up to that point. This way, the model keeps learning from new data while using the full history for better predictions.</p>

<p>In my experience, using an <strong>expanding walkforward approach</strong>, which includes all available data, works better than rolling validation methods. Rolling validation only uses recent data and may miss important long-term patterns, leading to less stable predictions. The expanding method allows the model to use more data, which helps it find these patterns and make more reliable predictions.</p>

<p>You can also choose to <strong>split the training data into a train and validation set</strong> within each period if you want to do hyperparameter tuning. This isn’t required for all cases, but it can help fine-tune the model before testing it, making it more accurate.</p>

<p>Below is a detailed representation of the expanding walkforward procedure:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: schematic overview of an expanding walk forward procedure.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>We use the same portfolio allocation model as in  <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">our previous article</a>. The strategy ranks stocks based on the model’s rankings, selecting the top 75 stocks to go long and the bottom 75 to go short.</p>

<p>To ensure the risk is balanced, we apply a volatility targeting approach. This approach adjusts the weights of stocks according to their risk profile to ensure that each stock within the portfolio contributes approximately the same amount of risk. This way, we achieve a balanced portfolio with consistent risk across the long and short portfolio.</p>

<p>By combining the model’s rankings with a risk-targeting strategy, we create a portfolio that is both well-diversified and dynamically adjust its risk over time.</p>

<h2 id="results">Results</h2>

<p>To analyze the impact of different modeling choices, I have plotted the cumulative returns of all strategies, rescaled to a volatility of 8% (Figure 2). This ensures a fair comparison by normalizing risk levels across strategies.</p>

<p>In total, we evaluate 10 different modeling choices, combining 5 normalization methods (Unprocessed, Z-score overall, Ranking overall, Z-score by sector, and Ranking by sector) with 2 target labels (SR 20 and Return 20). The goal is to assess how both the normalization approach and the choice of target label influence performance.</p>

<p>For now, I’ve left the strategy labels out—let’s keep some tension before diving into the details. As you can see, there is quite a bit of variation in performance across the different approaches. Below, I present the results and key insights into how these modeling choices affect strategy returns.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Performance of all different modelling choices. All lines are scaled to the same level of volatility.</p>

<p>Figure 2 explores the impact of different modeling choices on performance, measured in terms of Sharpe ratio. The first comparison looks at normalization methods, evaluating whether ranking, raw values, or z-scoring lead to better results. The second focuses on whether normalization is applied globally or within sectors. The final comparison examines whether training the model on return or Sharpe ratio yields superior outcomes.</p>

<p>The results make one thing clear: ranking within sectors and training the model on Sharpe ratio consistently lead to the best performance. However, the impact of the normalization method itself is less obvious. I initially expected both z-scoring and ranking to outperform raw normalization, but the results don’t show a decisive advantage for either approach.</p>

<p>To dig deeper, Figure 3 provides a more granular view by conditioning performance on the target label. Interestingly, when training on Sharpe ratio, both ranking and z-scoring produce significantly stronger results compared to raw normalization. However, when training on return, z-scoring surprisingly performs the worst, while ranking remains a strong choice.</p>

<p>This contrast suggests that the effectiveness of a normalization method depends heavily on the modeling objective. While z-scoring improves performance when optimizing for Sharpe ratio, it seems detrimental when the model is trained on return. Ranking, on the other hand, appears to be a robust choice across both scenarios, making it a reliable default.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Sharpe Ratio Performance Across Key Modeling Choices.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Cumulative return of different normalization methods conditioned on the target label. Performance are scaled to the same level of volatility.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann. %)</th>
      <th>Volatility (ann. %)</th>
      <th>Sharpe Ratio (ann.)</th>
      <th>Maximum Drawdown (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sr_zscore_by_sector</td>
      <td>10.8</td>
      <td>8.1</td>
      <td>1.3</td>
      <td>12.6</td>
    </tr>
    <tr>
      <td>combo</td>
      <td>8.4</td>
      <td>6.7</td>
      <td>1.3</td>
      <td>13.4</td>
    </tr>
    <tr>
      <td>sr_ranking_by_sector</td>
      <td>9.8</td>
      <td>8.0</td>
      <td>1.2</td>
      <td>12.9</td>
    </tr>
    <tr>
      <td>sr_zscore_globally</td>
      <td>10.1</td>
      <td>8.6</td>
      <td>1.2</td>
      <td>19.0</td>
    </tr>
    <tr>
      <td>sr_ranking_globally</td>
      <td>9.3</td>
      <td>8.5</td>
      <td>1.1</td>
      <td>16.9</td>
    </tr>
    <tr>
      <td>sr_raw_globally</td>
      <td>8.9</td>
      <td>8.5</td>
      <td>1.1</td>
      <td>16.1</td>
    </tr>
    <tr>
      <td>return_ranking_by_sector</td>
      <td>7.6</td>
      <td>7.6</td>
      <td>1.0</td>
      <td>14.9</td>
    </tr>
    <tr>
      <td>return_raw_globally</td>
      <td>7.2</td>
      <td>7.3</td>
      <td>1.0</td>
      <td>16.7</td>
    </tr>
    <tr>
      <td>return_ranking_globally</td>
      <td>7.5</td>
      <td>7.7</td>
      <td>1.0</td>
      <td>18.4</td>
    </tr>
    <tr>
      <td>return_zscore_by_sector</td>
      <td>6.6</td>
      <td>7.4</td>
      <td>0.9</td>
      <td>20.1</td>
    </tr>
    <tr>
      <td>return_zscore_globally</td>
      <td>5.5</td>
      <td>7.5</td>
      <td>0.7</td>
      <td>21.0</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Statistics of all different modelling choices. Ranked in ascending order based on sharpe ratio.</p>

<h2 id="todo">TODO</h2>
<ul>
  <li>results be a bit more in detail about the performance</li>
  <li>Add flowchart for walkfroward; explain it a bit more that you can do an extra validation procedure</li>
  <li>Maybe add the normaliation a bit more clear with a fomula</li>
  <li>Conclude</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[In the previous article, I introduced the low volatility factor and demonstrated how a simple volatility scaling approach can significantly boost performance. This time, I’m taking it a step further by combining multiple predictors using a multiple linear regression model. I’ll also explore key modeling choices, including normalization techniques and the choice of target variable for training.]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Boring Way to Make Money?</title><link href="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Boring Way to Make Money?" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4000/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<p>The <strong>low-volatility factor</strong> is a well-known and widely researched concept in factor investing. Simply put, it focuses on selecting stocks with smaller price swings. These more stable, often “boring” stocks tend to outperform their more volatile counterparts on a <strong>risk-adjusted</strong> basis. Now, you might be thinking—doesn’t higher risk always mean higher returns? Well, as with most things in life, not always. In this post, I’ll take a look at whether the conventional belief about risk and return still holds true.</p>

<p>I’ll show you how to construct a <strong>simple long-short portfolio</strong> using the low-volatility factor, drawing on the <strong>Russell 1000 universe</strong>. I’ll also explain how <strong>volatility targeting</strong> can further enhance your strategy’s performance.</p>

<h3 id="tradeable-universe"><strong>Tradeable Universe</strong></h3>

<p>Let’s start by defining our tradeable universe. For this, I’m using the <strong>Russell 1000</strong> index, which includes the largest 1000 companies in the U.S. Over time, I’ve excluded stocks trading below $5. The analysis spans from <strong>1995 to 2024</strong>, and I’ve visualized the number of tradeable stocks during this period, which hovers around 1000. In total, we have around <strong>3,300 different stocks</strong> in our universe, as stocks enter and exit over time. Most importantly, these are the <strong>point-in-time constituents</strong>, which help to avoid <strong>survivorship bias</strong>. I plotted the number of tradeable stocks in Figure 1, which naturally fluctuates around 1000.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Number of tradeable stocks over time.</p>

<h3 id="defining-the-low-volatility-factor"><strong>Defining the Low Volatility Factor</strong></h3>

<p>The <strong>low-volatility factor</strong> targets stocks that have historically demonstrated lower price fluctuations. To identify these stocks, I calculate volatility over three different time windows:</p>

<ul>
  <li><strong>5-day volatility</strong></li>
  <li><strong>10-day volatility</strong></li>
  <li><strong>21-day volatility</strong></li>
</ul>

<p>I use multiple time windows for a simple reason—it just works better. This combined approach smooths out short-term noise and provides a more robust measure of volatility. The average volatility across these timeframes is 33%, with the first quartile at 18%, the third quartile at 39%, and a median of 26%. I winsorized the data, capping the lower bound at 5% and the upper bound at 200%, which explains the small bump at the right tail of the distribution.</p>

<p>Below, you can see the distribution of annualized volatilities across <strong>3,300 stocks</strong> throughout the entire sample period. This distribution is positively skewed, resembling a <strong>log-normal</strong> pattern—most stocks have moderate volatility, but a smaller number exhibit high volatility. This suggests that while most stocks have volatilities between 18% and 39%, a smaller number of stocks exhibit high volatility.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Histogram showing the distribution of volatility across the tradeable universe.</p>

<p>As a first check, I computed the correlation with the raw annualized volatility and forward return and Sharpe ratio for the next 10 days. Interestingly, when using Pearson correlation, we find a 0.03 positive correlation meaning stocks with higher volatility leads to more return. However, when changing to Spearman , a more robust measure, the correlation drops to 0, indicating no relationship at all. For the Sharpe ratio, I find the opposite results: a negative correlation of -0.035 for Pearson, which strengthens to -0.04 when using Spearman. These numbers may seem small to anyone new to finance, but trust me—you get used to it after a while. The signal-to-noise ratio is very low.</p>

<h3 id="portfolio-bucketing"><strong>Portfolio Bucketing</strong></h3>

<p>To construct the portfolios, I performed a <strong>cross-sectional ranking</strong> of stocks based on their volatilities at each point in time. Specifically, I ranked all stocks in the universe by their volatility and mapped these ranks to a uniform <strong>0 to 1</strong> distribution. Then, I assigned each stock to one of five portfolios according to its percentile rank.</p>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at point $t$ based on its volatility, and $N$ be the total number of stocks at a given time. The normalized rank is computed as:</p>

\[\text{Rank Score}_i = \frac{r_{i,t}}{N}\]

<p>Using this score, stocks are bucketed into the following volatility-based portfolios:</p>

<ul>
  <li><strong>Portfolio 1</strong>: Lowest 10% of stocks ($ 0 \leq \text{Rank Score} &lt; 0.1 $) → <strong>Low volatility</strong></li>
  <li><strong>Portfolio 2</strong>: 10% to 20% of stocks ($ 0.1 \leq \text{Rank Score} &lt; 0.2 $)</li>
  <li><strong>Portfolio 3</strong>: 20% to 80% of stocks ($ 0.2 \leq \text{Rank Score} &lt; 0.8 $)</li>
  <li><strong>Portfolio 4</strong>: 80% to 90% of stocks ($ 0.8 \leq \text{Rank Score} &lt; 0.9 $)</li>
  <li><strong>Portfolio 5</strong>: Highest 10% of stocks ($ 0.9 \leq \text{Rank Score} \leq 1.0 $) → <strong>High volatility</strong></li>
</ul>

<p>This process ensures that each stock’s portfolio assignment is determined <strong>relative to the cross-sectional volatility distribution</strong> at that specific point</p>

<h3 id="rebalancing"><strong>Rebalancing</strong></h3>

<p>The portfolios are rebalanced <strong>weekly</strong> to reflect changes in volatility over time. For simplicity, I haven’t factored in transaction costs in this analysis.</p>

<h3 id="constructing-a-long-only-and-long-short-portfolio"><strong>Constructing a Long-Only and Long-Short Portfolio</strong></h3>

<p>Once the stocks are bucketed, I created two types of portfolios: <strong>equal-weighted</strong> and <strong>volatility-targeted</strong>.</p>

<h4 id="1-equal-weighted-portfolio"><strong>1. Equal-Weighted Portfolio</strong></h4>

<p>In the <strong>equal-weighted portfolio</strong>, all stocks are given equal weight, and I remain fully invested at all times. However, this creates a mismatch between the volatilities of the long and short positions, which I’ll explain further below. To construct the long-short portfolio, I simply take the difference between the low-volatility and high-volatility portfolios.</p>

<h4 id="2-volatility-targeted-portfolio"><strong>2. Volatility-Targeted Portfolio</strong></h4>

<p><strong>Volatility targeting</strong> adjusts the weight of each stock based on its volatility to stabilize the portfolio. Here’s how it works:</p>

<ol>
  <li><strong>Target Volatility</strong>: Set a target volatility of 20% annualized.</li>
  <li><strong>Compute Volatility Ratio</strong>: For each stock, calculate:
\(\text{vol_ratio} = \frac{\sigma_{target}}{\hat{\sigma}_{i,t}}\)
where:
    <ul>
      <li>$\sigma_{target} = 20\%$</li>
      <li>${\hat{\sigma}_{i,t}}$ is the stock’s ex-ante volatility estimate (e.g., a rolling 60-day standard deviation).</li>
    </ul>
  </li>
  <li>
    <p><strong>Adjust Equal Weights</strong>: Multiply the equal weight of each stock by its $\text{vol_ratio}$:
\(w_i = \text{equal_weight} \times \text{vol_ratio}\)</p>
  </li>
  <li><strong>Cap Individual Weights</strong>: Ensure no stock weight exceeds 0.05.</li>
  <li><strong>Portfolio Weight Constraint</strong>: Ensure that the portfolio weights total does not exceed 1, where 1 means being fully invested. In periods of high volatility, the total weight may be reduced to limit exposure. While this may seem like market timing, I view it more as <strong>dynamically adjusting risk</strong>—a technique commonly used in <strong>Trend following</strong> strategies.</li>
</ol>

<p>The resulting portfolio balances the long and short legs by dynamically adjusting stock weights to achieve the target volatility.</p>

<h3 id="performance-analysis"><strong>Performance Analysis</strong></h3>

<h4 id="equal-weighted-portfolio"><strong>Equal-Weighted Portfolio</strong></h4>

<p>The performance of the equal-weighted long-short portfolio is summarized below. The metrics visualized include return, volatility, and Sharpe ratio—all annualized. Interestingly, the return in the low-volatility portfolio is significantly higher than in the high-volatility portfolio. As expected, the lowest-volatility stocks exhibit the lowest volatility, while the highest-volatility stocks show the highest volatility. Consequently, the low-volatility portfolio exhibits significantly better <strong>risk-adjusted performance</strong>. However, the mismatch between the low-volatility and high-volatility portfolios results in less-than-optimal performance for the long-short portfolio.</p>

<p><strong>Performane Before Volatility Targeting</strong>:</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.svg" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Bar chart showing Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="improving-performance-with-volatility-targeting"><strong>Improving Performance with Volatility Targeting</strong></h3>

<p>As shown in the previous figure, volatility targeting adjusts portfolio weights to balance risk between the long and short legs, leading to more stable performance. After applying volatility scaling, volatility becomes more uniform across portfolios, reducing imbalances. This adjustment enhances the Sharpe ratio for both the long-short and long-only portfolios.</p>

<p>Figure 4 illustrates this effect, showing that the volatilities of the five portfolios are now much more aligned, eliminating the previous mismatch. As a result, the L/S portfolio improves its performance, benefiting from a more balanced risk distribution.</p>

<p>Figure 5 visualizes the performance of the long, short, and L/S portfolios, while Table 1 presents key financial performance metrics. All performance figures are reported before accounting for transaction costs.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.svg" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Bar chart showing Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (after volatility targeting).</p>

<h4 id="metrics-after-volatility-targeting"><strong>Metrics After Volatility Targeting</strong>:</h4>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Net Asset Value of the volatility-targeted portfolio (after volatility targeting).</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long</th>
      <th>Short</th>
      <th>Long-Short</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geometric Return (ann. %)</td>
      <td>11.1</td>
      <td>2.9</td>
      <td>7.7</td>
    </tr>
    <tr>
      <td>Volatility (ann. %)</td>
      <td>9.7</td>
      <td>9.7</td>
      <td>8.3</td>
    </tr>
    <tr>
      <td>Modified Sharpe Ratio (ann.)</td>
      <td>1.1</td>
      <td>0.3</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>Maximum Drawdown (%)</td>
      <td>29.5</td>
      <td>36.7</td>
      <td>33.6</td>
    </tr>
    <tr>
      <td>Maximum Time Under Water</td>
      <td>612.0</td>
      <td>1647.0</td>
      <td>944.0</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Portfolio statistics for the long, short and long/short portfolio the after volatility targeting.</p>

<p>Finally, Figure 6 visualizes the portfolio weights of the long (low volatility, 1) portfolio and the short (high volatility, 5) portfolio. Clearly, the portfolio weight for the high-volatility portfolio is substantially lower. It fluctuates between 0.2 and 0.6, while the portfolio weights for the low-volatility portfolio are almost always close to being fully invested. This makes sense since the high volatility portfolio will invest in stocks whith relatively high volatility while the low volatlity portfolio invest in stocks with low relatively volatility Only during significant events like the 2000 dot-com bubble, 2009, and the more recent COVID crisis do we see a sudden drop in portfolio weights due to higher volatility.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /></p>

<p><strong>Figure 6</strong>: Portfolio weights for Long and Short portfolios after volatility targeting.</p>

<h3 id="key-takeaways"><strong>Key Takeaways</strong></h3>

<ol>
  <li>A short-term low-volatility factor has historically delivered strong <strong>risk-adjusted returns</strong> (before transaction costs).</li>
  <li>Equal weighting in long-short portfolios leads to volatility imbalances, resulting in suboptimal performance.</li>
  <li>Incorporating a simple volatility-targeting approach effectively balances risk between long and short positions, significantly improving portfolio stability and performance.</li>
</ol>

<p>By incorporating a simple volatility-targeting approach, one can create a more robust and high-performing long-short portfolio. In the next blog, I’ll explore how to combine multiple factors in a portfolio.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quant" /><summary type="html"><![CDATA[The low-volatility factor is a well-known and widely researched concept in factor investing. Simply put, it focuses on selecting stocks with smaller price swings. These more stable, often “boring” stocks tend to outperform their more volatile counterparts on a risk-adjusted basis. Now, you might be thinking—doesn’t higher risk always mean higher returns? Well, as with most things in life, not always. In this post, I’ll take a look at whether the conventional belief about risk and return still holds true.]]></summary></entry></feed>